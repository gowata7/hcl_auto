import re
import time
from datetime import datetime, timedelta
from novaclient import client
import pandas as pd
from novaclient import client
from keystoneauth1 import session
from keystoneauth1.identity import v3
from odf.opendocument import OpenDocumentSpreadsheet
from odf.table import Table, TableRow, TableCell
from odf.text import P
from keystoneclient.v3 import client as keystone_client

import psycopg2
from psycopg2.extras import RealDictCursor

current_date = datetime.now().strftime("%Y%m%d")

def create_ods_file(file_path, header, data_rows):
    print(" ODS íŒŒì¼ ìƒì„± ì‹œì‘...")

    # Windows ê²½ë¡œ ë§ì¶¤ ì„¤ì •
    file_path = file_path.replace("/", "\\")

    #  ë§Œì•½ ë™ì¼í•œ íŒŒì¼ì´ ìˆìœ¼ë©´ ì‚­ì œ
    if os.path.exists(file_path):
        os.remove(file_path)

    #  ODS Document ìƒì„±
    doc = OpenDocumentSpreadsheet()
    table = Table(name="VM_Data")

    #  Header ì¶”ê°€
    print(f"ğŸ“ Header ì¶”ê°€ ì¤‘: {header}")
    header_row = TableRow()
    for column_name in header:
        cell = TableCell()
        # ğŸ‘‰ value-typeì„ stringìœ¼ë¡œ ëª…ì‹œ
        cell.setAttribute("valuetype", "string")
        cell.addElement(P(text=str(column_name)))
        header_row.addElement(cell)
    table.addElement(header_row)

    #  Data ì¶”ê°€
    print(f"ğŸ“ ì´ {len(data_rows)}ê°œì˜ ë°ì´í„°ê°€ ì¶”ê°€ë©ë‹ˆë‹¤.")
    for idx, row in enumerate(data_rows):
        # print(f"â¡ï¸ [{idx+1}/{len(data_rows)}] {row}")
        row_element = TableRow()
        for cell_data in row:
            if cell_data is None:
                cell_data = ""
            cell = TableCell()
            cell.setAttribute("valuetype", "string")
            cell.addElement(P(text=str(cell_data)))
            row_element.addElement(cell)
        table.addElement(row_element)

    # Tableì„ Spreadsheetì— ì¶”ê°€
    print(f"ğŸ’¾ Spreadsheetì— Table ì¶”ê°€")
    doc.spreadsheet.addElement(table)

    #  íŒŒì¼ ì§ì ‘ ì“°ê¸°
    print(f"ğŸ’¾ ODS íŒŒì¼ ì“°ê¸° ì‹œì‘... {file_path}")
    try:
        with open(file_path, "wb") as f:
            doc.write(f)      # ëª…ì‹œì ìœ¼ë¡œ write ìˆ˜í–‰
            f.flush()         # ë²„í¼ì— ë‚¨ì•„ìˆëŠ” ë‚´ìš© ë¹„ìš°ê¸°
        print(f"âœ… ODS íŒŒì¼ ìƒì„± ì™„ë£Œ: {file_path}")
    except Exception as e:
        print(f"âŒ íŒŒì¼ ì €ì¥ ì‹¤íŒ¨: {e}")

# íŒŒì¼ëª… ìƒì„± í•¨ìˆ˜
def get_file_and_topic_names(current_date):
    file_name = f'C:\\Beomjun\\csv\\VM\\vm_info_{current_date}.ods'
    return file_name

def nova_extract(project_name, vpn_name):

    if is_vpn_connected(vpn_name):
        print(f"[INFO] VPN {vpn_name} is already connected.")
    else:
        print(f"[INFO] VPN {vpn_name} not connected. Attempting connection...")
        connect_vpn(vpn_name, USERNAME, PASSWORD)

    auth = v3.Password(
        auth_url='http://cloud-control-vip.eu-central.openstack.h53:5000/v3',
        username='admin',
        password='TldhTl1!',
        user_domain_name='Default',
        project_name='admin',
        project_domain_name='Default'
    )

    sess = session.Session(auth=auth)
    nova = client.Client('2.1', session=sess)

    # ëª¨ë“  í”„ë¡œì íŠ¸ VM ê°€ì ¸ì˜¤ê¸°
    vms = nova.servers.list(search_opts={'all_tenants': 1})

    # í”„ë¡œì íŠ¸ ë¦¬ìŠ¤íŠ¸
    keystone = keystone_client.Client(session=sess)
    projects = keystone.projects.list()
    project_map = {p.id: p.name for p in projects}  # id â†’ name ë§µ ìƒì„±
    
    ods_file_path = get_file_and_topic_names(current_date)
    print("ODS íŒŒì¼ëª…:", ods_file_path)
    
    ods_header = [ "id", "project_id", "name", "status", "cpu", "memory", "flavor", "address", "availability_zone", "hostname", "created_at", "updated_at"]
    data_rows = []

    for vm in vms:
        project_name = project_map.get(vm._info.get('tenant_id'), "unknown")
        
        # ì£¼ì†Œ ì •ë³´ êµ¬ì„±
        addresses_info = ""
        for key, values in vm.addresses.items():
            for addr_info in values:
                addresses_info += f'{key}:{addr_info["addr"]}\n'
        addresses_info = addresses_info.strip()

        # Flavor ì •ë³´
        try:
            flavor_id = vm.flavor['id']
            flavor = nova.flavors.get(flavor_id)
            flavor_name = flavor.name
        except Exception as e:
            flavor_name = ""

        # CPUì™€ ë©”ëª¨ë¦¬ ì¶”ì¶œ
        cpu_match = re.search(r'(\d+)vCPU', flavor_name)
        mem_match = re.search(r'RAM(\d+)GB', flavor_name)
        cpu = int(cpu_match.group(1)) if cpu_match else None
        memory = int(mem_match.group(1)) if mem_match else None

        # AZ ê°’ ë³´ì •
        az = vm._info.get('OS-EXT-AZ:availability_zone')
        if not az or az.lower() == "none":
            if "prd" in addresses_info.lower():
                az = "PRD"
            elif "stg" in addresses_info.lower():
                az = "STG"
            else:
                az = "unknown"

        vm_data = [
            vm.id, project_name, vm.name, vm.status, cpu, memory,
            flavor_name, addresses_info, az,
            vm._info.get('OS-EXT-SRV-ATTR:host', ''),
            vm.created, vm.updated
        ]
        data_rows.append(vm_data)
        # print("data!", vm_data)

    # ODS íŒŒì¼ ìƒì„±
    create_ods_file(ods_file_path, ods_header, data_rows)
    return ods_file_path

# ì˜¤ë˜ëœ íŒŒì¼ ì •ë¦¬
def cleanup(tenants):
    time.sleep(1)
    
    retention_period = 5
    delete_date = (datetime.now() - timedelta(days=retention_period)).strftime("%Y%m%d")
    host_dir = 'C://Beomjun//csv//'
    file_list = os.listdir(host_dir)

    for tenant in tenants:
        for file_name in file_list:
            if file_name.startswith(f'vm_eu2_{tenant}_') and file_name <= f'vm_eu2_{tenant}_{delete_date}.ods':
                file_path = os.path.join(host_dir, file_name)
                os.remove(file_path)
                print("file remove success!")

def add_document_vm():
    from datetime import datetime, timedelta
    # ë‚ ì§œ ì„¤ì •
    yesterday_date = (datetime.now() - timedelta(days=1)).strftime("%Y%m%d")

    print("today:", current_date)
    print("yesterday:", yesterday_date)

    # ê²½ë¡œ ì„¤ì •
    today_path = f"C:\\Beomjun\\csv\\VM\\vm_info_{current_date}.ods"
    yesterday_path = f"C:\\Beomjun\\csv\\VM\\vm_info_{yesterday_date}.ods"

    # file_path=os.path.abspath("")
    # srcpath = findfile(f"eu_adminrc_{yesterday_date}.csv.org", file_path)
    # dir, file = os.path.split(srcpath)
    # shutil.copy2(srcpath, dir+f"\eu_adminrc_{yesterday_date}.csv")
        
    # CSV ë¡œë“œ
    df_today = pd.read_excel(today_path, engine='odf')
    df_yesterday = pd.read_excel(yesterday_path, engine='odf')

    # ID ê¸°ì¤€ìœ¼ë¡œ ì°¨ì§‘í•© ì—°ì‚°
    today_ids = set(df_today['id'])
    yesterday_ids = set(df_yesterday['id'])

    # ì¶”ê°€ëœ VM: ì˜¤ëŠ˜ì—ëŠ” ìˆê³  ì–´ì œëŠ” ì—†ë˜ ê²ƒ
    added_ids = today_ids - yesterday_ids
    df_added = df_today[df_today['id'].isin(added_ids)]

    # ì‚­ì œëœ VM: ì–´ì œëŠ” ìˆì—ˆëŠ”ë° ì˜¤ëŠ˜ì€ ì—†ëŠ” ê²ƒ
    deleted_ids = yesterday_ids - today_ids
    df_deleted = df_yesterday[df_yesterday['id'].isin(deleted_ids)]

    # ê²°ê³¼ ì¶œë ¥
    print("\nâœ… ì˜¤ëŠ˜ ì¶”ê°€ëœ VM:")
    print(df_added)

    print("\nâŒ ì˜¤ëŠ˜ ì‚­ì œëœ VM:")
    print(df_deleted)

    # for style in document.styles:
    #     print(style.name)
    
    return df_today, df_added, df_deleted

def get_this_week_range():
    from datetime import datetime, timedelta
    today = datetime.now().date()
    weekday = today.weekday()  # 0=ì›”, 1=í™”, ..., 6=ì¼

    # ì „ì£¼ ë™ì¼ ìš”ì¼ë¶€í„° ì˜¤ëŠ˜ê¹Œì§€
    start_date = today - timedelta(days=7)
    end_date = today

    return start_date, end_date

def fetch_weekly_added_vms():
    start_date, end_date = get_this_week_range()
    conn = psycopg2.connect(
        dbname="postgres",
        user="postgres",
        password="1234",
        host="localhost",
        port="5432"
    )
    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        cur.execute("""
            SELECT 
                project_id, instance_name, flavor, address, availability_zone, hostname,
                make_date(year::int, month::int, day::int) AS date
            FROM added_instances
            WHERE make_date(year::int, month::int, day::int) BETWEEN %s AND %s
            ORDER BY date;
        """, (start_date, end_date))
        rows = cur.fetchall()
    conn.close()
    return rows

def fetch_weekly_deleted_vms():
    start_date, end_date = get_this_week_range()
    conn = psycopg2.connect(
        dbname="postgres",
        user="postgres",
        password="1234",
        host="localhost",
        port="5432"
    )
    with conn.cursor(cursor_factory=RealDictCursor) as cur:
        cur.execute("""
            SELECT 
                project_id, instance_name, flavor, address, availability_zone, hostname,
                make_date(year::int, month::int, day::int) AS date
            FROM deleted_instances
            WHERE make_date(year::int, month::int, day::int) BETWEEN %s AND %s
            ORDER BY date;
        """, (start_date, end_date))
        rows = cur.fetchall()
    conn.close()
    return rows

def add_document_table():
    
    df_today, df_added, df_deleted = add_document_vm()

    document.add_heading('VM í˜„í™©', level=1) 
    total = len(df_today)

    document.add_paragraph(f'ì´ {total}ê±´', style='List Bullet')

    # Pivoting
    df = preprocess_df(df_today)
    # pivot, total = getPivotTable(df, month)

    # Chart - Pie ì°¨íŠ¸
    region_pivot, _ = getPivotTable_new(df, 'Region')
    tenant_pivot, _ = getPivotTable_new(df, 'Tenant')

    source1 = region_pivot
    source2 = tenant_pivot

    print(source1)
    print(source2)

    # Chart
    # if incompleted == 0:
    chart1 = getPieChart_region(source1)
    chart2 = getPieChart_tenant(source2)
    # else:
    #     source = flatten_2d(data)
    #     chart = getStackedHBarChart(source)    

    # source = pivot
    # if incompleted == 0:
    #     chart = getPieChart(source)
    # else:
    #     chart = getStackedHBarChart1(source)
    chart1.save(f'./charts/vm_1.png')
    chart2.save(f'./charts/vm_2.png')

    table = document.add_table(rows=1, cols=2)

    # ----------- ì¢Œì¸¡ ì…€ (ë¦¬ì „ë³„) ------------
    cell = table.rows[0].cells[0]
    paragraph = cell.paragraphs[0]
    paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER  # ìˆ˜í‰ ì¤‘ì•™ ì •ë ¬
    run = paragraph.add_run("ë¦¬ì „ë³„")
    run.font.size = Pt(12)  # í°íŠ¸ í¬ê¸° ì¡°ì ˆ (ì„ íƒ)
    run.bold = True  # êµµê²Œ (ì„ íƒ)

    # ì´ë¯¸ì§€ ì¶”ê°€ (ë³„ë„ ë¬¸ë‹¨ ìƒì„±í•˜ì—¬ ì•„ë˜ë¡œ ì •ë ¬ë˜ê²Œ)
    paragraph_img = cell.add_paragraph()
    paragraph_img.alignment = WD_ALIGN_PARAGRAPH.CENTER
    run_img = paragraph_img.add_run()
    run_img.add_picture('./charts/vm_1.png', width=Inches(3.3))

    # ----------- ìš°ì¸¡ ì…€ (í…Œë„ŒíŠ¸ë³„) ------------
    cell = table.rows[0].cells[1]
    paragraph = cell.paragraphs[0]
    paragraph.alignment = WD_ALIGN_PARAGRAPH.CENTER
    run = paragraph.add_run("í…Œë„ŒíŠ¸ë³„")
    run.font.size = Pt(12)
    run.bold = True

    paragraph_img = cell.add_paragraph()
    paragraph_img.alignment = WD_ALIGN_PARAGRAPH.CENTER
    run_img = paragraph_img.add_run()
    run_img.add_picture('./charts/vm_2.png', width=Inches(3.3))

    # Document
    document.add_paragraph(f'ì „ì¼ ëŒ€ë¹„ ì¶”ê°€ëœ VM({len(df_added)}ê±´)', style='List Bullet')
    # A-B (ì‚­ì œëœ ê±´)
    # df_result_sub = pd.concat([df_premonth,df_curmonth,df_curmonth]).drop_duplicates(keep=False)
    # print(df_result_sub)

    if len(df_added) == 0:
        pass
    else:
        addTable3(df_added)
        
    p = document.add_paragraph('')
    run = p.add_run()
    run.add_break(WD_BREAK.LINE)

    document.add_paragraph(f'ì „ì¼ ëŒ€ë¹„ ì‚­ì œëœ VM({len(df_deleted)}ê±´)', style='List Bullet')
    # A-B (ì‚­ì œëœ ê±´)
    # df_result_sub = pd.concat([df_premonth,df_curmonth,df_curmonth]).drop_duplicates(keep=False)
    # print(df_result_sub)

    if len(df_deleted) == 0:
        pass
    else:
        addTable3(df_deleted)

    p = document.add_paragraph('')
    run = p.add_run()
    run.add_break(WD_BREAK.LINE)

    start_date, end_date = get_this_week_range()

    fetch_weekly_added_vms()
    weekly_vms_added = fetch_weekly_added_vms()
    df_weekly_added = pd.DataFrame(weekly_vms_added)
    print("weekly_vms_added:", df_weekly_added)
    document.add_paragraph(f'1ì£¼ ê°„({start_date}~{end_date}) ìƒì„±ëœ VM ëª©ë¡ ({len(df_weekly_added)}ê±´)', style='List Bullet')
    
    if weekly_vms_added:
        addTable_instance(df_weekly_added)
    else:
        document.add_paragraph('1ì£¼ ê°„ ìƒì„±ëœ VMì´ ì—†ìŠµë‹ˆë‹¤.')
    
    p = document.add_paragraph('')
    run = p.add_run()
    run.add_break(WD_BREAK.LINE)

    fetch_weekly_deleted_vms()
    weekly_vms_deleted = fetch_weekly_added_vms()
    df_weekly_deleted = pd.DataFrame(weekly_vms_deleted)
    print("weekly_vms_deleted:", df_weekly_deleted)
    document.add_paragraph(f'1ì£¼ ê°„({start_date}~{end_date}) ì‚­ì œëœ VM ëª©ë¡ ({len(df_weekly_deleted)}ê±´)', style='List Bullet')
    
    if weekly_vms_deleted:
        addTable_instance(df_weekly_deleted)
    else:
        document.add_paragraph('1ì£¼ ê°„ ì‚­ì œëœ VMì´ ì—†ìŠµë‹ˆë‹¤.')

    document.add_page_break()
    # document.save(f'C:\\Beomjun\\csv\\vm_report_{current_date}.docx')


if __name__ == "__main__":
    from datetime import datetime, timedelta
    from lb_5 import *
    from x_config import *

    project_names = ['admin']
    tenants = ['adminrc']

    # ê° í”„ë¡œì íŠ¸ì— ëŒ€í•´ í•¨ìˆ˜ ì‹¤í–‰
    for project_name in project_names:
        print("This project is", project_name)
        ods_file_path = nova_extract(project_name, VPN_FR2_NAME)
    # cleanup(tenants)

    add_document_vm()
    add_document_table()

    ods_file = f"C:\\Beomjun\\csv\\LB\\lb_info_{current_date}.ods"

    print("[INFO] Starting collection for FR2")
    fr2_data = collect_lb_info("FR2", VPN_FR2_NAME, ip_list_fr2)

    print("[INFO] Starting collection for FR7")
    fr7_data = collect_lb_info("FR7", VPN_FR7_NAME, ip_list_fr7)

    # âœ… ë°ì´í„° ë³‘í•©
    combined_data = fr2_data + fr7_data
    columns = ["Tenant", "NS_IP", "LB_Name", "LB_VIP", "Port", "Service_Type"]
    df_combined = pd.DataFrame(combined_data, columns=columns)

    # # âœ… ODS íŒŒì¼ ì €ì¥
    with pd.ExcelWriter(ods_file, engine='odf') as writer:
        df_combined.to_excel(writer, sheet_name="LB_Info", index=False)

    df_added, df_deleted = compare_yesterday()
    add_document_lb(df_combined, df_added, df_deleted)
    
    print(f"\nâœ… ODS íŒŒì¼ì´ ìƒì„±ë˜ì—ˆìŠµë‹ˆë‹¤: {ods_file}")
